{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsBM0fs-qS6M"
   },
   "source": [
    "**Recommender systems**\n",
    "\n",
    "Recommender systems are one of the most commonly used practical systems in data science. In this section, we will focus on collaborative filtering, where the focus is on similarities between users. Depending on the past preference of users, this type of recommender system recommends items that users have liked or rated highly in the past. For this task, we will use Surprise, a Python scikit-learn library for building and analyzing recommender systems.\n",
    "\n",
    "We first need to read the merged df into Surprise, set the rating scale of the dataset, and load data from df into Surprise data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZNCh8TEqOGN"
   },
   "outputs": [],
   "source": [
    "# Set rating scale of the dataset\n",
    "reader = Reader(rating_scale=(0, 2))\n",
    "\n",
    "# Load the dataframe with ratings.\n",
    "data = Dataset.load_from_df(df[['userID', 'placeID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wV-6izlqhU2"
   },
   "source": [
    "Now, we are set and can use the Surprise library functionalities. First, we will get a benchmark on this dataset from different available algorithms in Surprise. We will do cross-validation on the whole dataset and append the results in bencmark_scores. We also set random seed to 114 to get reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4XTaTVDqifq"
   },
   "outputs": [],
   "source": [
    "benchmark_scores = []\n",
    "\n",
    "random.seed(114)\n",
    "np.random.seed(114)\n",
    "\n",
    "# Iterate selected algorithms\n",
    "for algorithm in [BaselineOnly(), SVD(), SVDpp(), KNNWithMeans(),KNNWithZScore(), CoClustering(), NMF()]:\n",
    " # Cross-validation\n",
    " cv = cross_validate(algorithm, data, cv=5, verbose=False)\n",
    "\n",
    " # create df with cv results\n",
    " df_cv = pd.DataFrame.from_dict(cv).mean(axis=0)\n",
    " df_cv = df_cv.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    " benchmark_scores.append(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIYVq6K7qpNf"
   },
   "source": [
    "Now, let's create a pandas DataFrame from the benchmark_scores list and see the results of each algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRcyQqlKqsUp"
   },
   "outputs": [],
   "source": [
    "# Create results DataFrame from the benckmark scores\n",
    "results = pd.DataFrame(benchmark_scores).set_index('Algorithm').sort_values('test_rmse')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2BpgnJ5qwD8"
   },
   "source": [
    "results are shown as follows. Each algorithm and its result is displayed in DataFrame. In this particular dataset, the SVDpp algorithm, which is part of the matrix factorization recommender within the collaborative filtering algorithms, performs well. It has a Root Mean Squared Error (RMSE) of 0.65. The next best algorithm, in this case, happens to be the KNNWithMeans algorithm, directly derived from a basic nearest neighbors approach, with an RMSE of 0.66:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OECvBt0HqzkU"
   },
   "source": [
    "We will apply these two algorithms, SVDpp and KNNWithMeans, into the dataset, and compare how they perform in different scenarios. First, let's set up cross-validation with 5 splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRZSOpifq2JH"
   },
   "outputs": [],
   "source": [
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffqJgqSlq4uc"
   },
   "source": [
    "Then, we will also set the two chosen algorithms, SVDpp and KNNwithMeans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdVdP55Gq9gG"
   },
   "outputs": [],
   "source": [
    "# Define algorithms \n",
    "algo_knnwithMeans = KNNWithMeans()\n",
    "algo_svdpp = SVDpp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H06z_T5erBPa"
   },
   "source": [
    "Let's start with the KNNWithMeans algorithm and apply it to our dataset.\n",
    "\n",
    "**KNNWithMeans**\n",
    "\n",
    "KNNWithMeans is a basic collaborative filtering algorithm, taking into account the mean ratings of each user. It is inspired directly by k-nearest neighborhood and the main tuning parameter is the maximum number of k. We will use the default, which is 40, but you can try and see how this changes the results.\n",
    "\n",
    "First, we set a seed to make the results reproducible. Then, we loop through the data and split into training and test datasets according to the kf cross-validation we have created previously. Inside the loop, we first fit on the training dataset, then predict with the test dataset. We will calculate accuracy using RMSE metrics. Finally, we dump the data into a pandas DataFrame for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9y1T-L1rFeK"
   },
   "outputs": [],
   "source": [
    "random.seed(114)\n",
    "np.random.seed(114)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "   # train and test algorithm with KNNWithMeans.\n",
    "    algo_knnwithMeans.fit(trainset)\n",
    "    predictions_knnwithmeans = algo_knnwithMeans.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions_knnwithmeans, verbose=True)\n",
    "\n",
    "    dump.dump('./dump_KNNWithMeans', predictions_knnwithmeans, algo_knnwithMeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzCNsRjDrLnx"
   },
   "source": [
    "The preceding fits on the training dataset, predicts on testset, and calculates accuracy based on RMSE metrics for each iteration. We can get the mean of RMSE for all iterations by averaging all RMSEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxcETyvHrOE9"
   },
   "outputs": [],
   "source": [
    "print(\"Average RMSE of the CV is: \", np.mean(accuracy.rmse(predictions_knnwithmeans, verbose=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3LbSyH1rR3R"
   },
   "source": [
    "Now, let's load the dumped file of this algorithm and look closely at the predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZTUUiACrV-E"
   },
   "outputs": [],
   "source": [
    "# Load the dump file\n",
    "predictions_knnwithmeans, algo_knnwithMeans = dump.load('./dump_KNNWithMeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr0g0WFsrY1U"
   },
   "source": [
    "After loading the dumped file, we can easily create a DataFrame from it like this and look at the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgXUI4qbrcbp"
   },
   "outputs": [],
   "source": [
    "df_knnithmeans = pd.DataFrame(predictions_knnwithmeans, columns=['uid', 'iid', 'rui', 'est', 'details']) \n",
    "df_knnithmeans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBxyLrV2rgG7"
   },
   "source": [
    "This df_knnithmeans consists of five columns. The first one, uid, as you might recognize, is userID from our dataset; the second column, iid, is placeID of the restaurants. rui represents the rating of users with items. est is the estimated or predicted result from the algorithm. To calculate the error of each row, we can simply subtract the rui and est columns. Let's create a column called err to store the error results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANdFT-BDrhgI"
   },
   "outputs": [],
   "source": [
    "# Calculate the error\n",
    "df_knnithmeans['err'] = abs(df_knnithmeans.est - df_knnithmeans.rui)\n",
    "df_knnithmeans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab6WAqj5rnNI"
   },
   "source": [
    "We will move to an SVDpp algorithm and later come back to compare the results of these two different algorithms. \n",
    "\n",
    "**SVDpp**\n",
    "\n",
    "The SVDpp algorithm is an extension of the SVD algorithm popularized by the fact that it won third place in the Netflix recommendation competition. SVDpp takes into account implicit rating, which is an improvement on the original SVD algorithm. We will carry out the same procedure as before but only change the algorithm from KNNwithMeans to SVDpp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFTBzHVjrs5H"
   },
   "outputs": [],
   "source": [
    "random.seed(114)\n",
    "np.random.seed(114)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    # train and test algorithm with SVDpp.\n",
    "    algo_svdpp.fit(trainset)\n",
    "    predictions_svdpp = algo_svdpp.test(testset)\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions_svdpp, verbose=True)\n",
    "    # Dump the prediction into dataframe\n",
    "    dump.dump('./dump_SVDpp', predictions_svdpp, algo_svdpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF0VBrWsryAf"
   },
   "source": [
    "Let's print out the average RMSE of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnlU1kpArzMp"
   },
   "outputs": [],
   "source": [
    "print(\"Average RMSE of the CV is: \", np.mean(accuracy.rmse(predictions_svdpp, verbose=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mE79zFLr7QJ"
   },
   "source": [
    "This is an improvement from the KNNWithMeans result.\n",
    "\n",
    "Let's also load the dumped file and create DataFrame with the results. We will also calculate the error of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uarAnhF_r85_"
   },
   "outputs": [],
   "source": [
    "# Load the dump file\n",
    "predictions_svdpp, algo_svdpp = dump.load('./dump_SVDpp')\n",
    "\n",
    "df_svdpp = pd.DataFrame(predictions_svdpp, columns=['uid', 'iid', 'rui', 'est', 'details'])  \n",
    "\n",
    "\n",
    "df_svdpp['err'] = abs(df_svdpp.est - df_svdpp.rui)\n",
    "df_svdpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QclioekgsEkv"
   },
   "source": [
    "Although we can see that SVDpp performs better than KNNwithMeans in this case, we can compare these two algorithms to find out where each performs better than the other. \n",
    "\n",
    "***Comparison and interpretations***\n",
    "\n",
    "We can simply get the worst predictions of the algorithms by sorting them. Let's first get the worst predictions of df_knnithmeans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWpTXWMJumLD"
   },
   "outputs": [],
   "source": [
    "df_knnithmeans.sort_values(by='err')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09bnCHQ5uqgz"
   },
   "source": [
    "We will do the same for df_svdpp to get the worst 10 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h_m-eiCusCH"
   },
   "outputs": [],
   "source": [
    "df_svdpp.sort_values(by='err')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6ZJMqhluvxF"
   },
   "source": [
    "And here is the output of the worst 10 predictions for df_svdpp. Compared to the preceding table, you can see that this table has lower error rates. The worst prediction error is 1.79, compared to 2.00 from the preceding table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fhpEeRuy0H"
   },
   "source": [
    "We can show the overall distribution of the prediction errors in distplot using seaborn. We will construct a 2 x 2 plot, where we will plot the prediction errors of df_svdpp and df_knnithmeans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1txVZ7BAu3PL"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,8))\n",
    "sns.distplot(df_svdpp.err, ax=ax[0])\n",
    "sns.distplot(df_knnithmeans.err, ax=ax[1])\n",
    "ax[0].set_title('SVDpp')\n",
    "ax[1].set_title('KNNwithMeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIQdJ-heu6sQ"
   },
   "source": [
    "It seems both algorithms are right-skewed and have higher predictions around zero. So, what happens when a user has a smaller number of ratings? Remember that we had a mean rating of 1.19. Let's choose users with fewer than 5 ratings. This function is copied from the documentation of the Surprise library and calculates the number of items rated by a given user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifhEQ6YZu8ju"
   },
   "outputs": [],
   "source": [
    "def get_Iu(uid):\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAE63SqkvGEF"
   },
   "source": [
    "We will use this function to calculate the user rating for both df_knnithmeans and df_svdpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSlgogalvJEB"
   },
   "outputs": [],
   "source": [
    "df_knnithmeans['Iu'] = df_knnithmeans.uid.apply(get_Iu)\n",
    "df_svdpp['Iu'] = df_svdpp.uid.apply(get_Iu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw4YTZ2rvL-v"
   },
   "source": [
    "We can now compare the error rates of the two DataFrame when users only have fewer than 5 ratings and see which one of the algorithms performs better. We will use the mean here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVOcWxTHvOjp"
   },
   "outputs": [],
   "source": [
    "df_knnithmeans[df_knnithmeans.Iu < 5].err.mean(), \n",
    "df_svdpp[df_svdpp.Iu < 5].err.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmTUXvpGvRyR"
   },
   "source": [
    "This is for df_knnithmeans and df_svdpp respectively. It seems that KNNwithMeans is performing much better than SVDpp when users have fewer ratings.\n",
    "\n",
    "In the next section, we will cover location-based (LB) recommenders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liZRpeNmvUkB"
   },
   "source": [
    "***Location-based recommenders***\n",
    "\n",
    "LB recommenders include explication location components to provide more relevant recommendations based on the location of users or items. We will carry out a simple LB recommendation using the k-means clustering techniques we covered in Chapter 4, Making Sense of Humongous Location Datasets. \n",
    "\n",
    "We will first fit on a small sample of df and get the labels. Let's also print out the number of k and silhouette_score, which is the metric we will use and the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcT5RoGfvYqF"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=24, init='k-means++')\n",
    "X_sample = df[['longitude','latitude']].sample(frac=0.1)\n",
    "kmeans.fit(X_sample)\n",
    "y = kmeans.labels_\n",
    "\n",
    "print(\"k = 24\", \" silhouette_score \", silhouette_score(X_sample, y, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiO9h6eSvew_"
   },
   "source": [
    "Our k value is equal to k = 24 and silhouette_score, which is 0.5461956922155007.\n",
    "\n",
    "Now, we will predict on the whole dataset and populate in a new column in our df['cluster'], let's also get a sample of 10 rows from df:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl76GufQvkb-"
   },
   "outputs": [],
   "source": [
    "df['cluster'] = kmeans.predict(df[['longitude','latitude']])\n",
    "df[['userID','latitude','longitude','placeID','cluster']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkAG8bUfvp8y"
   },
   "source": [
    "We will use top-rated venues for our recommendation. This can be changed to, for example, the number of rating per restaurant, or more tailored recommenders such as different cuisines of restaurants. Let's get the highest rated restaurants. Here, we will not only include rating but also food_rating and service_rating since we have many restaurants with a rating of 2. We call this topvenues_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uaw851pvtpF"
   },
   "outputs": [],
   "source": [
    "topvenues_df = df.sort_values(by=['rating', 'food_rating', 'service_rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgYxWruMvyev"
   },
   "source": [
    "We create a simple function that receives df, as well as latitude and longitude. The function first predicts the cluster of coordinates provided. Once it gets the cluster number, it passes through df provided in this topvenues_df and gets the first top-rated name of the restaurant in this cluster. Finally, the function prints out recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edPvJy0pv3qe"
   },
   "outputs": [],
   "source": [
    "def recommend_restaurants(df, longitude, latitude):\n",
    "    # Predict the cluster for longitude and latitude provided\n",
    "    cluster = kmeans.predict(np.array([longitude,latitude]).reshape(1,-1))[0]\n",
    "\n",
    "    # Get the best restaurant in this cluster\n",
    "    name = df[df['cluster']==cluster].iloc[0]['name']\n",
    "\n",
    "    print('\"{}\" is recommended'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ_bmUmUv-Pl"
   },
   "source": [
    "Let's also create a function that plots a Folium map for both user location and restaurant locations. This function takes df, a user coordinates, and the restaurant name produced by the preceding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAubdeISxgEV"
   },
   "outputs": [],
   "source": [
    "def create_folium_map(df, user_coords, restaurant_name):\n",
    "    m = folium.Map(\n",
    "    location=user_coords,\n",
    "    zoom_start=10,\n",
    "    tiles='Stamen Terrain'\n",
    "    )\n",
    "\n",
    "    folium.Marker(\n",
    "    location=user_coords,\n",
    "    popup='User Location',\n",
    "    icon=folium.Icon(icon='cloud')\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "    folium.Marker(\n",
    "    location=list(df[df['name'] == restaurant_name][['latitude', 'longitude']].iloc[0]),\n",
    "    popup='Restaurant Location',\n",
    "    icon=folium.Icon(color='red',icon='info-sign')\n",
    "    ).add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKaj36Y9xmLk"
   },
   "source": [
    "Let's use the recommend_restaurants function to recommend restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbgvN-IkxwhI"
   },
   "outputs": [],
   "source": [
    "recommend_restaurants(topvenues_df,-99.145185, 23.730925)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7WoRRk-x55x"
   },
   "source": [
    "Here is another example with different locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz9Yw9V8x8Ix"
   },
   "outputs": [],
   "source": [
    "recommend_restaurants(topvenues_df, -100.939752, 22.150849)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inwf_eJpyCEP"
   },
   "source": [
    "We utilize the create_folium_map function to display user location and restaurant location in a map for the last example, which has recommended \"Rincon Huasteco\". Let's first create variables that hold user_coords and restaurant_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyo-r3VLyD_e"
   },
   "outputs": [],
   "source": [
    "user_coords = [22.120849, -100.839752]\n",
    "restaurant_name = \"Rincon Huasteco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_aEKMvyyI8m"
   },
   "source": [
    "Now, let's pass these variables to the create_folium_map function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC4L6QPIyK15"
   },
   "outputs": [],
   "source": [
    "create_folium_map(df, user_coords, restaurant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S_JUHt6yOCk"
   },
   "source": [
    "Congratulations! This is just the beginning of your journey in geospatial data science. While reading this book, you have been introduced to a broad and essential range of geospatial Python libraries, as well as real-world applications. I hope that this will be your inspiration to continue learning and working on the vast array of geospatial data science projects out there."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNp0Ur8N4aLV8HEQx9sxdmC",
   "collapsed_sections": [],
   "name": "LocationRecommenderSystems(10b).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
