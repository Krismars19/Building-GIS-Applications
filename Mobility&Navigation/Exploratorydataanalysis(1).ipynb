{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exploratorydataanalysis(1).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPjLCSAG0jivoniwTJHj9f3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kF8ro6qqRnNU"},"source":["**Exploratory data analysis for NY Taxi fare**"]},{"cell_type":"code","metadata":{"id":"rLeYN9CjRexO"},"source":["df = pd.read_csv(\"NYC_sample.csv\")\n","df.head().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oN1jynsOSW6C"},"source":["Handling missing values"]},{"cell_type":"code","metadata":{"id":"udnx5UDBSahL"},"source":["# Determine the percentage of missing values in each column of the data\n","\n","na_counts = pd.DataFrame(df.isna().sum()/len(df)) \n","na_counts.columns = [\"null_row_pct\"]\n","na_counts[na_counts.null_row_pct > 0].sort_values(by = \"null_row_pct\", ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLjaG_8DTh_V"},"source":["Removing the missing location ID and coordinates"]},{"cell_type":"code","metadata":{"id":"KVdUv0etT-NX"},"source":["df = df[~(\n"," (df.Dropoff_latitude.isna()) & (df.DOLocationID.isna())\n",")]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpx0PP62Us0o"},"source":["Handling time values"]},{"cell_type":"markdown","metadata":{"id":"YeyOMJw8U6Dt"},"source":["Converting the pickup and dropoff times into pandas datetime values to calculate the target value, which will be the natural log of the difference in time between dropoff and pickup in seconds"]},{"cell_type":"code","metadata":{"id":"AKcf2SyRUxeU"},"source":["df[\"trip_duration\"] = np.log((df.Lpep_dropoff_datetime - df.lpep_pickup_datetime).dt.seconds + 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1xtx-wFVT6W"},"source":["In the preceding line of code, we are adding 1 second to the trip duration to prevent an undefined error when a log transformation is applied over the value.\n","\n","But why are we using natural log transformation over the trip duration? There are three reasons for this, as follows:\n","\n","For the Kaggle competition on New York taxi trip duration prediction, the evaluation metric is defined as the Root Mean Squared Logarithmic Error (RMSLE). When log transformation is applied and the RMSE is calculated over the target values, we get the RMSLE. This helps us compare our results with the best-performing teams. \n","\n","Errors in log scale let us know by how many factors we were wrong, for example, whether we were 10% off from the actual values or 70% off. We will be discussing this in detail when we look at the Error metric section.\n","\n","The log transformation over the target variable follows a perfectly normal distribution. This satisfies one of the assumptions of linear regression. The plot of the trip duration values (on a log scale) looks as follows:"]},{"cell_type":"markdown","metadata":{"id":"GBh5ZpiZVEZv"},"source":["**Time values as a feature**\n","\n","The pickup time (pickup_datetime) can be considered a feature, and can be further deconstructedinto disparate components such as day of the week, month, hour, minute, and Boolean indicators, such as whether the datetime is a weekday or not, as shown in the following screenshot.add_datepart() is a convenience method in thefastai.structured module adds most of these components for us:"]},{"cell_type":"code","metadata":{"id":"ZKhqQ3UgVI6t"},"source":["add_datepart(df, 'lpep_pickup_datetime', time=True)\n","df.tail().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GaO040wUVPsa"},"source":["**Handling unrelated data**\n","\n","There are a few features that aren't related to the value to be predicted, in our case things such as fare and vendor ID and the dropoff time. Therefore, we will go ahead and drop these columns:"]},{"cell_type":"code","metadata":{"id":"Tz7aN9vtVPFA"},"source":["drop_columns = [\n","\"Ehail_fee\", \"Extra\", \"Payment_type\", \"Total_amount\", \"improvement_surcharge\", \"Tolls_amount\", \"Tip_amount\", \"MTA_tax\", \"VendorID\", \"RateCodeID\", \"Store_and_fwd_flag\", \"Fare_amount\", \"Lpep_dropoff_datetime\", 'Trip_type ', 'Passenger_count'\n","]\n","df.drop(columns=drop_columns, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P22rJVQiVpI-"},"source":["**Spatial data processing**\n","\n","Three things in this section: taxi zones, spatial joins, and the calculation of distances.\n","\n","Taxi zones in New York\n","\n","Analyzing and processing a taxi zone spatial data helps us achieve two objectives: \n","\n","- Substitute the missing coordinates for pickup and dropoff locations with the taxi zone's centroid\n","\n","- Use the taxi zone as a feature in the model"]},{"cell_type":"markdown","metadata":{"id":"CbLZdgoKWWL6"},"source":["**Visualization of taxi zones**\n","\n","We have provided the shapefile for the taxi zones in the data repository. Shapefiles can be read as (Geo)DataFrames with the Python library known as GeoPandas, like so:"]},{"cell_type":"code","metadata":{"id":"_0Kf1rw8WbXY"},"source":["taxi_zones = gpd.read_file(\"taxi_zones.shp\")\n","taxi_zones.tail().T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MeMEe3tMWual"},"source":["# Visualize taxi zones with the plot method of the GeoDataFrame\n","\n","#Projecting Taxi Zones into WGS84 coordinate system\n","taxi_zones = taxi_zones.to_crs({'init': 'epsg:4326'})\n","\n","#Plot the Geodataframe\n","ax = taxi_zones.plot(column = \"zone\", figsize = (12, 12), alpha = 0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lnx9hEKeZUES"},"source":["**Spatial joins**\n","\n","If we can derive the pickup and dropoff taxi zone of each trip, we can add these as features to our machine learning model. For this, we need to perform an operation known as a spatial join, which is nothing but a Point-in-Polygon solution that's supported by the GeoDataFrame. The following code has an assign_taxi_zones() function, which takes our DataFrame as an input and returns a pandas series. Internally, it does three things: \n","\n","- Construct a GeoDataFrame using the input DataFrame's latitude and longitude values (point geometry)\n","- Perform a spatial join between the point and the taxi zones (polygon geometry)\n","- Return the location ID of the taxi zone for each coordinate"]},{"cell_type":"code","metadata":{"id":"4kWwytMUZfRz"},"source":["from shapely.geometry import Point\n","\n","def assign_taxi_zones(df, lon_var, lat_var, locid_var):\n","    try:\n","        # Construct a Geodataframe using the coordinates of each trip\n","        local_gdf = gpd.GeoDataFrame(\n","            crs={'init': 'epsg:4326'},\n","            geometry=[Point(xy) for xy in\n","                      zip(df[lon_var], df[lat_var])])                     \n","\n","        #Perform a spatial join with the Taxi Zones\n","        local_gdf = gpd.sjoin(local_gdf, taxi_zones, how='left', op='within')\n","\n","        return local_gdf.LocationID.rename(locid_var)\n","    except ValueError as ve:\n","        print(ve)\n","        print(ve.stacktrace())\n","        series = df[lon_var]\n","        series = np.nan\n","        return series\n","\n","\"\"\"\n","Calculate pickup and dropoff taxi zone ids\n","\"\"\"\n","\n","df['pickup_taxizone_id'] = assign_taxi_zones(df, \"Pickup_longitude\",\"Pickup_latitude\", \"pickup_taxizone_id\")\n","\n","df['dropoff_taxizone_id'] = assign_taxi_zones(df, \"Dropoff_longitude\",\"Dropoff_latitude\",\"dropoff_taxizone_id\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkxgGk6BZss5"},"source":["Remember, this operation will only assign a taxi zone that we know the coordinates of.\n","\n","To backfill the missing coordinates with the centroid of the taxi zone, we can follow these steps: \n","\n","- Find the centroid of each taxi zone\n","- Join the DataFrame with the taxi zones based on the pickup zone ID that we just computed (pickup_taxizone_id)\n","- Transfer the taxi zone's centroid to the DataFrame\n","- For all rows with missing pickup coordinates, substitute the centroid values\n","- Apply the same process in order to backfill missing dropoff coordinates"]},{"cell_type":"code","metadata":{"id":"C1o5VyL-Z4my"},"source":["#1. Finding Taxi Zone' Centroid\n","taxi_zones[\"X\"] = taxi_zones.centroid.x\n","taxi_zones[\"Y\"] = taxi_zones.centroid.y\n","\n","#2. Join dataframe with taxizone based on pickup zone id\n","df = pd.merge(df, taxi_zones[[\"LocationID\",\"X\", \"Y\"]], how = \"left\", left_on = \"PULocationID\", right_on = \"LocationID\")\n","\n","#3.Substitute missing lat/long values w/ \n","# the taxi zone's centroid\n","\n","df.Pickup_longitude.fillna(df.X, inplace=True)\n","df.Pickup_latitude.fillna(df.Y, inplace=True)\n","\n","\n","df.drop(columns=[\"LocationID\", \"X\", \"Y\"], inplace=True)\n","\n","#5. Apply same process for Dropoff zone\n","df = pd.merge(df, taxi_zones[[\"LocationID\",\"X\", \"Y\"]], how = \"left\", left_on = \"DOLocationID\", right_on = \"LocationID\")\n","\n","df.Dropoff_longitude.fillna(df.X, inplace=True)\n","df.Dropoff_latitude.fillna(df.Y, inplace=True)\n","\n","df.drop(columns=[\"LocationID\", \"X\", \"Y\"], inplace=True)\n","\n","df.tail().T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nh1x4XgsZ7cY"},"source":["We can use a very similar process to add the borough names from the taxi zone shapefile to each row in the DataFrame. In this process, we have added the following features:\n","\n","- Pickup zone ID\n","- Dropoff zone ID\n","- Pickup borough\n","- Dropoff borough\n","\n","We were also quite successful in backfilling many missing values in pickup and dropoff location coordinates, as well as taxi zone IDs."]},{"cell_type":"markdown","metadata":{"id":"nxw8laOhaSKf"},"source":["**Calculating distances**\n","\n","When it comes to distance, there are different kinds of distance that make sense in this context. These are as follows:\n","\n","- Distance as the crow flies (or Haversine distance) \n","- Distance as you drive in Manhattan (or Manhattan distance)\n","\n","**Haversine distance**\n","Haversine distance is the Great Circle Distance (GCD) between two geographic coordinates. A GCD is incidentally the shortest distance between the two coordinates. This is almost similar to a Euclidean distance (or a straight-line distance), except that we are accounting for the spherical nature of the Earth (yes, we are generalizing the Earth as a sphere with a radius of 3,958 miles to make our lives easier). The Python code for calculating the Haversine distance is as follows:"]},{"cell_type":"code","metadata":{"id":"mBchX6-UaZr9"},"source":["import numpy as np\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 3958.76 # Earth radius in miles\n","    dLat = np.radians(lat2 - lat1)\n","    dLon = np.radians(lon2 - lon1)\n","    lat1 = np.radians(lat1)\n","    lat2 = np.radians(lat2)\n","    a = np.sin(dLat/2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dLon/2) ** 2\n","    c = 2*np.arcsin(np.sqrt(a))\n","    return R * c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYPeCEh2arnW"},"source":["The preceding function takes a pair of coordinates (the latitude and longitude values of the source and destination, respectively) and returns the Haversine distance between them. "]},{"cell_type":"markdown","metadata":{"id":"Y_UFPH8jate1"},"source":["**Manhattan distance**\n","\n","Manhattan distance is a distance metric inspired by the near rectangular street block design in Manhattan. The distance between two locations is calculated as the sum of the straight-line distance along the x axis and the straight-line distance along the y axis"]},{"cell_type":"markdown","metadata":{"id":"OGh5NYm-a3pE"},"source":["The following operations are required to calculate the Manhattan distance when given pickup and dropoff coordinates: \n","\n","- Perform a matrix multiplication of the pickup and dropoff coordinates with the rotation matrix, where θ = -29º\n","- Derive the hinge point coordinates as per the formula provided\n","- Transform the hinge point back into the geographic coordinate system by performing a rotation of an equal amount in an anti-clockwise direction ( θ = +29º)\n","- Use the preceding formula to calculate the Manhattan distance using the pickup, hinge, and dropoff coordinates"]},{"cell_type":"code","metadata":{"id":"h67aoOwAa_k6"},"source":["theta1 = np.radians(-28.904)\n","theta2 = np.radians(28.904)\n","R1 = np.array([[np.cos(theta1), np.sin(theta1)], [-np.sin(theta1), np.cos(theta1)]])\n","R2 = np.array([[np.cos(theta2), np.sin(theta2)], [-np.sin(theta2), np.cos(theta2)]])\n","\n","def manhattan_dist(lat1, lon1, lat2, lon2):\n","    p = np.stack([lat1, lon1], axis = 1)\n","    d = np.stack([lat2, lon2], axis = 1)\n","    pT = R1 @ p.T \n","    dT = R1 @ d.T \n","\n","    vT = np.stack((pT[0,:], dT[1,:]))\n","    v = R2 @ vT\n","    return (haversine(p.T[0], p.T[1], v[0], v[1]) + haversine(v[0], v[1], d.T[0], d.T[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBukooHab_dd"},"source":["For now, let's just add the first two types of distance as different columns to the training DataFrame"]},{"cell_type":"code","metadata":{"id":"mBAwSbOncA4O"},"source":["df[\"haversine_dist\"] = haversine(df[\"pickup_latitude\"], df[\"pickup_longitude\"], \\\n"," df[\"dropoff_latitude\"], df[\"dropoff_longitude\"])\n","\n","df[\"manhattan_dist\"] = manhattan_dist(df[\"pickup_latitude\"], df[\"pickup_longitude\"], \\\n"," df[\"dropoff_latitude\"], df[\"dropoff_longitude\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVPRCHScckPY"},"source":["**Error metric**\n","\n","If we visit the evaluation section of the Kaggle competition, the evaluation metric is defined as the RMSLE. In the competition, the objective is to minimize this metric for the test data. An error is simply the difference between actual values and predicted values:\n","\n","error = predicted value - actual value\n","\n","The Root Mean Squared Error (RMSE) would literally be the square root applied over the mean of all the squared error terms for each observation.\n","\n","However, our metric in the Kaggle competition needs to be a log error:\n","\n","log_error = log(predicted value + 1) - log(actual value + 1)\n","\n","Therefore, it is important to apply a log transform over the trip_duration column as we did earlier:"]},{"cell_type":"code","metadata":{"id":"6xFqTxExcoPT"},"source":["df[\"trip_duration\"] = np.log(df[\"trip_duration\"] + 1) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dcjge9izcs25"},"source":["Now, we can use a function that can calculate RMSE rather a function that calculates RMSLE:"]},{"cell_type":"code","metadata":{"id":"FwP7qiCvcvGB"},"source":["import math \n","def rmse(x,y): return math.sqrt(((x-y)**2).mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Fyaq6ISc63Q"},"source":["**Interpreting errors** \n","\n","What does an RMSLE of, say, 0.3 actually mean? Well, let's visit the formula for the log error again:\n","\n","log_error = 0.3\n","\n","log(predicted_value + 1) - log(actual_value + 1) = 0.3\n","\n","\n","log((predicted_value + 1) / (actual_value + 1)) = 0.3\n","\n","(predicted_value + 1) / (actual_value + 1) = 1.349\n","\n","predicted_value = 1.349 * actual_value +0.349\n","\n","If the preceding derivation is hard to follow, it doesn't matter; it's just for math enthusiasts. What this means is that, on average, our naive model predicts the trip duration, 1.35 times the actual value. This is not too bad, given that the best model in the Kaggle competition predicts, on average, 1.3 times the actual value. We can arrive at this metric by using a single line of Python code: "]},{"cell_type":"code","metadata":{"id":"5fBJKl--c-8S"},"source":["np.exp(rmsle)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TzCifeVydG0V"},"source":["The response to the preceding line of code is the factor by which our predictive model is off from the actual values."]},{"cell_type":"markdown","metadata":{"id":"6wIGlLMrdTYL"},"source":["**Building the model**\n","\n","Let's build the final model using a random forest regressor. A random forest is a universal machine learning technique, that is, it can handle different kinds of data; it could be a category (classification), a continuous variable (regression), or features of any kind, such an image, price, time, post codes, and so on (that is, both structured and unstructured data). It doesn't generally overfit too much, and it is very easy to stop it from overfitting. For these reasons, random forest is a versatile ML technique which we can effectively use to solve our problem.\n","\n","**Validation data and error metrics**\n","Our initial step is choosing a suitable size for validation data. Before delineating the validation dataset and defining accuracy metrics, we have just two more steps to take into account that will make our data ready for building models. These are two convenience functions that are provided by fastai to make our models more robust:\n","\n","- train_cats(): Convert any string data into categorical data\n","- proc_df(): Perform one-hot encoding on categorical variables and handle missing values\n","\n","Let's have a look at the following code snippet:"]},{"cell_type":"code","metadata":{"id":"XuNGKw_Bdf0o"},"source":["train_cats(train_df)\n","tdf, y, nas = proc_df(df, 'trip_duration')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgRSbH3vdie7"},"source":["A validation data size of 20,000 will be enough to validate our model:"]},{"cell_type":"code","metadata":{"id":"zM5CROIGdnO_"},"source":["def split_vals(a,n): \n","    return a[:n].copy(), a[n:].copy()\n","\n","n_valid = 20000\n","n_trn = len(tdf)-n_valid\n","raw_train, raw_valid = split_vals(df, n_trn)\n","X_train, X_valid = split_vals(tdf, n_trn)\n","y_train, y_valid = split_vals(y, n_trn)\n","\n","X_train.shape, y_train.shape, X_valid.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWPtMsd-dprm"},"source":["We will use RMSE and R2 as accuracy metrics. RMSE is a very simple yet effective measure to understand errors, and R2 is a very effective metric to evaluate the predictive power of a model. In the Kaggle competition, we talked about the top RMSE values, which are around 0.28 at the time of writing"]},{"cell_type":"code","metadata":{"id":"XPar49kxdtQC"},"source":["def rmse(x,y): return np.sqrt(((x-y)**2).mean())\n","\n","def print_score(m):\n","    res = f\"\"\"Train RMS : {rmse(m.predict(X_train), y_train)}, \n","            Valid RMSE : {rmse(m.predict(X_valid), y_valid)},\n","            Train R2 score : {m.score(X_train, y_train)}, \n","            Valid R2 score: {m.score(X_valid, y_valid)}\n","           \"\"\"\n","    if hasattr(m, 'oob_score_'): res += f\" OOB Score : {m.oob_score_}\"\n","    print(res)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3kUmCTedyjd"},"source":["Let's go ahead and run our first model with about 40 estimators (trees). We will be using all the CPUs that are available to us to enable multiprocessing in the background (hence the n_jobs = -1 parameter)"]},{"cell_type":"code","metadata":{"id":"G23U5eR7d20P"},"source":["m = RandomForestRegressor(n_estimators = 40, n_jobs=-1, oob_score=True)\n","m.fit(X_train, y_train)\n","print_score(m)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLWnVxNoeIoO"},"source":["The preceding model gives us these scores. A validation RMSE of 0.25 indicates that we are among the 5th percentile of competitors"]},{"cell_type":"code","metadata":{"id":"XjdidHsSe2YR"},"source":["Train RMS : 0.09880475531046715, \n","Valid RMSE : 0.2599229455143868,\n","Train R2 score : 0.9786976405479708, \n","Valid R2 score: 0.8602128925971101\n","OOB Score : 0.8480746084914459"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYcos7bQfCzd"},"source":["Let's see if we can make the model any better. There are some cool methods in fastai that let us understand the importance of the features that are used in the model, as well as the correlation among the features (multicollinearity). For example, we can easily look at the important features in our model by using the rf_feature_importance() method"]},{"cell_type":"code","metadata":{"id":"MXLQDGJdg1ip"},"source":["fi = rf_feat_importance(m, df_trn); fi[:20]feature_imp\n","def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(16,8), legend=False, grid = False)\n","\n","plot_fi(fi[:20]);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iikqv5r9g7rQ"},"source":["We can also write our own code to assess conditions such as multicollinearity, in which two features are closely associated with one another. The following lines of code plot a dendrogram, which shows multicollinearity between features"]},{"cell_type":"code","metadata":{"id":"C-oCNFPig_sb"},"source":["from scipy.cluster import hierarchy as hc\n","\n","corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\n","corr_condensed = hc.distance.squareform(1-corr)\n","z = hc.linkage(corr_condensed, method='average')\n","fig = plt.figure(figsize=(12,10))\n","dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qx8HNkkChEIE"},"source":["The dendrogram shows highly correlated features, such as Dayofyear and datetimeElapsed, as well as datetimeMonth. The analysis also shows high correlation between the Manhattan and Haversine distances\n","\n","Once we remove such redundant features and introduce the regularization parameter (max_features = 0.5), our model's RMSE drops even further, so it's among the top 2%ile of the leaderboard!"]}]}